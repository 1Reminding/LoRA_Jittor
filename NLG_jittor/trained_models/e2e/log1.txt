| epoch   1 step      100 |    100 batches | lr 4e-05 | ms/batch 541.41 | loss  5.23 | avg loss  5.56 | ppl 260.63
| epoch   1 step      200 |    200 batches | lr 8e-05 | ms/batch 538.58 | loss  3.15 | avg loss  3.76 | ppl 42.86
| epoch   1 step      300 |    300 batches | lr 0.00012 | ms/batch 539.59 | loss  2.80 | avg loss  3.15 | ppl 23.38
| epoch   1 step      400 |    400 batches | lr 0.00016 | ms/batch 540.48 | loss  2.94 | avg loss  2.97 | ppl 19.59
| epoch   1 step      500 |    500 batches | lr 0.0002 | ms/batch 540.30 | loss  2.92 | avg loss  2.91 | ppl 18.44
| epoch   1 step      600 |    600 batches | lr 0.000198 | ms/batch 542.33 | loss  2.81 | avg loss  2.93 | ppl 18.68
| epoch   1 step      700 |    700 batches | lr 0.000196 | ms/batch 539.62 | loss  3.13 | avg loss  2.87 | ppl 17.58
| epoch   1 step      800 |    800 batches | lr 0.000193 | ms/batch 540.89 | loss  2.91 | avg loss  2.81 | ppl 16.58
| epoch   1 step      900 |    900 batches | lr 0.000191 | ms/batch 541.36 | loss  3.02 | avg loss  2.81 | ppl 16.64
| epoch   1 step     1000 |   1000 batches | lr 0.000189 | ms/batch 541.23 | loss  2.77 | avg loss  2.80 | ppl 16.41
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.1000.pt
| epoch   1 step     1100 |   1100 batches | lr 0.000187 | ms/batch 541.43 | loss  2.57 | avg loss  2.74 | ppl 15.53
| epoch   1 step     1200 |   1200 batches | lr 0.000184 | ms/batch 540.19 | loss  3.20 | avg loss  2.79 | ppl 16.31
| epoch   1 step     1300 |   1300 batches | lr 0.000182 | ms/batch 540.38 | loss  2.97 | avg loss  2.73 | ppl 15.34
| epoch   1 step     1400 |   1400 batches | lr 0.00018 | ms/batch 540.28 | loss  2.46 | avg loss  2.75 | ppl 15.65
| epoch   1 step     1500 |   1500 batches | lr 0.000178 | ms/batch 541.09 | loss  2.91 | avg loss  2.74 | ppl 15.53
| epoch   1 step     1600 |   1600 batches | lr 0.000175 | ms/batch 540.88 | loss  2.73 | avg loss  2.74 | ppl 15.46
| epoch   1 step     1700 |   1700 batches | lr 0.000173 | ms/batch 541.04 | loss  2.75 | avg loss  2.65 | ppl 14.22
| epoch   1 step     1800 |   1800 batches | lr 0.000171 | ms/batch 541.57 | loss  2.41 | avg loss  2.70 | ppl 14.94
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.1893.pt
start to train the model................ 2
| epoch   2 step     1900 |      7 batches | lr 0.000169 | ms/batch 35.48 | loss  3.27 | avg loss  2.90 | ppl 18.21
| epoch   2 step     2000 |    107 batches | lr 0.000167 | ms/batch 543.59 | loss  2.85 | avg loss  2.72 | ppl 15.17
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.2000.pt
eval samples: 0 loss: jt.Var([1.00191], dtype=float32)
eval samples: 100 loss: jt.Var([1.8544922], dtype=float32)
eval samples: 200 loss: jt.Var([1.7841756], dtype=float32)
eval samples: 300 loss: jt.Var([1.0364195], dtype=float32)
eval samples: 400 loss: jt.Var([1.4633853], dtype=float32)
average loss 1.3697681188583375
----------------------------------------------------------------------------------------------------
| Eval   1 at step     2000 | time: 62.58s | valid loss  1.37 | valid ppl  3.93 | best ppl  3.93 
----------------------------------------------------------------------------------------------------
| epoch   2 step     2100 |    207 batches | lr 0.000164 | ms/batch 1167.23 | loss  2.52 | avg loss  2.70 | ppl 14.94
| epoch   2 step     2200 |    307 batches | lr 0.000162 | ms/batch 539.79 | loss  2.97 | avg loss  2.74 | ppl 15.42
| epoch   2 step     2300 |    407 batches | lr 0.00016 | ms/batch 543.69 | loss  2.65 | avg loss  2.68 | ppl 14.55
| epoch   2 step     2400 |    507 batches | lr 0.000158 | ms/batch 538.26 | loss  2.93 | avg loss  2.66 | ppl 14.30
| epoch   2 step     2500 |    607 batches | lr 0.000155 | ms/batch 540.69 | loss  2.71 | avg loss  2.71 | ppl 15.01
| epoch   2 step     2600 |    707 batches | lr 0.000153 | ms/batch 540.72 | loss  2.60 | avg loss  2.69 | ppl 14.76
| epoch   2 step     2700 |    807 batches | lr 0.000151 | ms/batch 542.45 | loss  3.14 | avg loss  2.65 | ppl 14.15
| epoch   2 step     2800 |    907 batches | lr 0.000149 | ms/batch 542.87 | loss  2.37 | avg loss  2.68 | ppl 14.61
| epoch   2 step     2900 |   1007 batches | lr 0.000146 | ms/batch 541.74 | loss  2.49 | avg loss  2.65 | ppl 14.20
| epoch   2 step     3000 |   1107 batches | lr 0.000144 | ms/batch 541.19 | loss  2.60 | avg loss  2.63 | ppl 13.93
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.3000.pt
| epoch   2 step     3100 |   1207 batches | lr 0.000142 | ms/batch 542.25 | loss  2.79 | avg loss  2.67 | ppl 14.51
| epoch   2 step     3200 |   1307 batches | lr 0.00014 | ms/batch 540.94 | loss  2.46 | avg loss  2.63 | ppl 13.83
| epoch   2 step     3300 |   1407 batches | lr 0.000138 | ms/batch 540.62 | loss  2.86 | avg loss  2.65 | ppl 14.20
| epoch   2 step     3400 |   1507 batches | lr 0.000135 | ms/batch 540.87 | loss  2.31 | avg loss  2.65 | ppl 14.10
| epoch   2 step     3500 |   1607 batches | lr 0.000133 | ms/batch 540.97 | loss  2.52 | avg loss  2.65 | ppl 14.16
| epoch   2 step     3600 |   1707 batches | lr 0.000131 | ms/batch 540.57 | loss  2.71 | avg loss  2.59 | ppl 13.37
| epoch   2 step     3700 |   1807 batches | lr 0.000129 | ms/batch 540.99 | loss  2.98 | avg loss  2.60 | ppl 13.44
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.3786.pt
start to train the model................ 3
| epoch   3 step     3800 |     14 batches | lr 0.000126 | ms/batch 73.13 | loss  2.39 | avg loss  2.67 | ppl 14.39
| epoch   3 step     3900 |    114 batches | lr 0.000124 | ms/batch 540.20 | loss  2.58 | avg loss  2.66 | ppl 14.31
| epoch   3 step     4000 |    214 batches | lr 0.000122 | ms/batch 540.37 | loss  2.80 | avg loss  2.63 | ppl 13.81
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.4000.pt
eval samples: 0 loss: jt.Var([1.031135], dtype=float32)
eval samples: 100 loss: jt.Var([1.7933136], dtype=float32)
eval samples: 200 loss: jt.Var([1.6792256], dtype=float32)
eval samples: 300 loss: jt.Var([0.9864438], dtype=float32)
eval samples: 400 loss: jt.Var([1.3521706], dtype=float32)
average loss 1.3051882403947057
----------------------------------------------------------------------------------------------------
| Eval   2 at step     4000 | time: 52.32s | valid loss  1.31 | valid ppl  3.69 | best ppl  3.69 
----------------------------------------------------------------------------------------------------
| epoch   3 step     4100 |    314 batches | lr 0.00012 | ms/batch 1064.23 | loss  2.81 | avg loss  2.67 | ppl 14.45
| epoch   3 step     4200 |    414 batches | lr 0.000117 | ms/batch 541.81 | loss  2.50 | avg loss  2.61 | ppl 13.54
| epoch   3 step     4300 |    514 batches | lr 0.000115 | ms/batch 540.00 | loss  2.15 | avg loss  2.58 | ppl 13.26
| epoch   3 step     4400 |    614 batches | lr 0.000113 | ms/batch 539.63 | loss  2.51 | avg loss  2.66 | ppl 14.31
| epoch   3 step     4500 |    714 batches | lr 0.000111 | ms/batch 540.48 | loss  2.75 | avg loss  2.63 | ppl 13.91
| epoch   3 step     4600 |    814 batches | lr 0.000109 | ms/batch 540.24 | loss  2.85 | avg loss  2.58 | ppl 13.23
| epoch   3 step     4700 |    914 batches | lr 0.000106 | ms/batch 540.39 | loss  2.56 | avg loss  2.63 | ppl 13.85
| epoch   3 step     4800 |   1014 batches | lr 0.000104 | ms/batch 539.86 | loss  2.22 | avg loss  2.61 | ppl 13.63
| epoch   3 step     4900 |   1114 batches | lr 0.000102 | ms/batch 540.04 | loss  2.49 | avg loss  2.57 | ppl 13.12
| epoch   3 step     5000 |   1214 batches | lr 9.96e-05 | ms/batch 540.35 | loss  2.51 | avg loss  2.61 | ppl 13.64
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.5000.pt
| epoch   3 step     5100 |   1314 batches | lr 9.74e-05 | ms/batch 541.12 | loss  2.53 | avg loss  2.58 | ppl 13.20
| epoch   3 step     5200 |   1414 batches | lr 9.51e-05 | ms/batch 540.07 | loss  2.47 | avg loss  2.59 | ppl 13.38
| epoch   3 step     5300 |   1514 batches | lr 9.29e-05 | ms/batch 540.66 | loss  2.52 | avg loss  2.60 | ppl 13.50
| epoch   3 step     5400 |   1614 batches | lr 9.07e-05 | ms/batch 540.56 | loss  2.47 | avg loss  2.61 | ppl 13.61
| epoch   3 step     5500 |   1714 batches | lr 8.85e-05 | ms/batch 540.13 | loss  3.00 | avg loss  2.55 | ppl 12.74
| epoch   3 step     5600 |   1814 batches | lr 8.62e-05 | ms/batch 539.56 | loss  2.49 | avg loss  2.55 | ppl 12.76
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.5679.pt
start to train the model................ 4
| epoch   4 step     5700 |     21 batches | lr 8.4e-05 | ms/batch 110.84 | loss  2.41 | avg loss  2.62 | ppl 13.73
| epoch   4 step     5800 |    121 batches | lr 8.18e-05 | ms/batch 539.83 | loss  2.54 | avg loss  2.61 | ppl 13.56
| epoch   4 step     5900 |    221 batches | lr 7.95e-05 | ms/batch 539.98 | loss  2.53 | avg loss  2.58 | ppl 13.22
| epoch   4 step     6000 |    321 batches | lr 7.73e-05 | ms/batch 540.70 | loss  2.87 | avg loss  2.64 | ppl 13.99
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.6000.pt
eval samples: 0 loss: jt.Var([0.92672724], dtype=float32)
eval samples: 100 loss: jt.Var([1.7500895], dtype=float32)
eval samples: 200 loss: jt.Var([1.6420231], dtype=float32)
eval samples: 300 loss: jt.Var([0.9589765], dtype=float32)
eval samples: 400 loss: jt.Var([1.2937688], dtype=float32)
average loss 1.2775264346173831
----------------------------------------------------------------------------------------------------
| Eval   3 at step     6000 | time: 52.74s | valid loss  1.28 | valid ppl  3.59 | best ppl  3.59 
----------------------------------------------------------------------------------------------------
| epoch   4 step     6100 |    421 batches | lr 7.51e-05 | ms/batch 1068.12 | loss  2.44 | avg loss  2.55 | ppl 12.81
| epoch   4 step     6200 |    521 batches | lr 7.28e-05 | ms/batch 540.76 | loss  2.72 | avg loss  2.56 | ppl 12.95
| epoch   4 step     6300 |    621 batches | lr 7.06e-05 | ms/batch 540.79 | loss  2.63 | avg loss  2.61 | ppl 13.60
| epoch   4 step     6400 |    721 batches | lr 6.84e-05 | ms/batch 542.46 | loss  2.16 | avg loss  2.58 | ppl 13.25
| epoch   4 step     6500 |    821 batches | lr 6.61e-05 | ms/batch 541.62 | loss  2.74 | avg loss  2.56 | ppl 12.91
| epoch   4 step     6600 |    921 batches | lr 6.39e-05 | ms/batch 540.92 | loss  2.43 | avg loss  2.59 | ppl 13.35
| epoch   4 step     6700 |   1021 batches | lr 6.17e-05 | ms/batch 541.11 | loss  2.50 | avg loss  2.57 | ppl 13.12
| epoch   4 step     6800 |   1121 batches | lr 5.95e-05 | ms/batch 541.11 | loss  3.05 | avg loss  2.54 | ppl 12.71
| epoch   4 step     6900 |   1221 batches | lr 5.72e-05 | ms/batch 541.92 | loss  2.40 | avg loss  2.57 | ppl 13.12
| epoch   4 step     7000 |   1321 batches | lr 5.5e-05 | ms/batch 540.36 | loss  2.21 | avg loss  2.53 | ppl 12.60
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.7000.pt
| epoch   4 step     7100 |   1421 batches | lr 5.28e-05 | ms/batch 539.34 | loss  2.23 | avg loss  2.57 | ppl 13.04
| epoch   4 step     7200 |   1521 batches | lr 5.05e-05 | ms/batch 540.67 | loss  2.19 | avg loss  2.57 | ppl 13.08
| epoch   4 step     7300 |   1621 batches | lr 4.83e-05 | ms/batch 541.65 | loss  2.13 | avg loss  2.57 | ppl 13.01
| epoch   4 step     7400 |   1721 batches | lr 4.61e-05 | ms/batch 541.51 | loss  2.61 | avg loss  2.52 | ppl 12.46
| epoch   4 step     7500 |   1821 batches | lr 4.38e-05 | ms/batch 541.15 | loss  2.46 | avg loss  2.50 | ppl 12.15
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.7572.pt
start to train the model................ 5
| epoch   5 step     7600 |     28 batches | lr 4.16e-05 | ms/batch 149.09 | loss  2.76 | avg loss  2.59 | ppl 13.36
| epoch   5 step     7700 |    128 batches | lr 3.94e-05 | ms/batch 540.62 | loss  2.55 | avg loss  2.57 | ppl 13.07
| epoch   5 step     7800 |    228 batches | lr 3.71e-05 | ms/batch 540.29 | loss  2.55 | avg loss  2.55 | ppl 12.87
| epoch   5 step     7900 |    328 batches | lr 3.49e-05 | ms/batch 540.40 | loss  3.07 | avg loss  2.61 | ppl 13.58
| epoch   5 step     8000 |    428 batches | lr 3.27e-05 | ms/batch 540.87 | loss  2.45 | avg loss  2.51 | ppl 12.33
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.8000.pt
eval samples: 0 loss: jt.Var([0.91094977], dtype=float32)
eval samples: 100 loss: jt.Var([1.73724], dtype=float32)
eval samples: 200 loss: jt.Var([1.6428614], dtype=float32)
eval samples: 300 loss: jt.Var([0.9328521], dtype=float32)
eval samples: 400 loss: jt.Var([1.2737232], dtype=float32)
average loss 1.2634819538110778
----------------------------------------------------------------------------------------------------
| Eval   4 at step     8000 | time: 52.71s | valid loss  1.26 | valid ppl  3.54 | best ppl  3.54 
----------------------------------------------------------------------------------------------------
| epoch   5 step     8100 |    528 batches | lr 3.05e-05 | ms/batch 1067.91 | loss  2.74 | avg loss  2.54 | ppl 12.64
| epoch   5 step     8200 |    628 batches | lr 2.82e-05 | ms/batch 540.87 | loss  2.44 | avg loss  2.57 | ppl 13.07
| epoch   5 step     8300 |    728 batches | lr 2.6e-05 | ms/batch 540.53 | loss  2.89 | avg loss  2.57 | ppl 13.03
| epoch   5 step     8400 |    828 batches | lr 2.38e-05 | ms/batch 540.83 | loss  3.04 | avg loss  2.53 | ppl 12.59
| epoch   5 step     8500 |    928 batches | lr 2.15e-05 | ms/batch 540.73 | loss  2.63 | avg loss  2.55 | ppl 12.78
| epoch   5 step     8600 |   1028 batches | lr 1.93e-05 | ms/batch 540.86 | loss  2.60 | avg loss  2.55 | ppl 12.86
| epoch   5 step     8700 |   1128 batches | lr 1.71e-05 | ms/batch 541.34 | loss  2.66 | avg loss  2.52 | ppl 12.42
| epoch   5 step     8800 |   1228 batches | lr 1.48e-05 | ms/batch 542.15 | loss  2.25 | avg loss  2.54 | ppl 12.64
| epoch   5 step     8900 |   1328 batches | lr 1.26e-05 | ms/batch 541.84 | loss  2.47 | avg loss  2.52 | ppl 12.41
| epoch   5 step     9000 |   1428 batches | lr 1.04e-05 | ms/batch 541.21 | loss  2.44 | avg loss  2.56 | ppl 12.87
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.9000.pt
| epoch   5 step     9100 |   1528 batches | lr 8.14e-06 | ms/batch 540.98 | loss  3.08 | avg loss  2.53 | ppl 12.59
| epoch   5 step     9200 |   1628 batches | lr 5.91e-06 | ms/batch 541.58 | loss  2.26 | avg loss  2.54 | ppl 12.69
| epoch   5 step     9300 |   1728 batches | lr 3.68e-06 | ms/batch 541.65 | loss  2.59 | avg loss  2.49 | ppl 12.12
| epoch   5 step     9400 |   1828 batches | lr 1.45e-06 | ms/batch 540.93 | loss  2.58 | avg loss  2.50 | ppl 12.16
saving checkpoint /root/autodl-tmp/trained_models/GPT2_M/e2e_jittor/model.9465.pt